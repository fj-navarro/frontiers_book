---
title: "Big Data"
---

The progress made by internet technologies and the widespread adoption of mobile devices have made it possible for users today to generate vast amounts of data, also known as Big Data. Ordinary people in their daily activities generate an avalanche of information, which spans different types of sources, from social networks (e.g., Facebook, Twitter, Instagram, and YouTube), review platforms (e.g., Tripadvisor), reservation systems (e.g., Booking.com, Hotels.com), blogs, discussion forums, collaborative maps, and many others (Peterlin et al., 2021). This massive user-generated content (UGC) data, along with other passive sources of data generated from devices (DGD), is one of the trends that is having the greatest impact in today's business context.

Big Data has become a core organizational asset and a crucial competitive factor in any tourism business strategy. Consequently, tourism organizations around the world are seeking innovative new techniques to help them maximize the potential of Big Data and address the new challenges created. Historically, IT departments managed the systems in which business transaction data (e.g., orders, sales, shipping, inventory) was generated, stored, and processed. But things have changed a lot in the last few years. The amount and variety of data being generated, how data is now processed, and what can be done with it, are being made possible by new advanced data analytics techniques tailored to large amounts of data (Larson & Chang, 2016). These changes in the treatment of data are affecting practically all industries at an ever-accelerating pace of innovation. For example, some of the most recent developments focus on analyzing the mood of tourists and determining their state of mind (i.e., when a new product or service is introduced or when tourists have a problem). This gives an idea of the depth of the changes that are taking place and how firms are trying to find new answers to who, when, and where.

Business owners and managers must be aware that the earthquake caused by Big Data is not a spontaneous event. In fact, it is part of a transition that is taking place on a global scale in economies, which involves the metamorphosis of value creation from tangible assets to intangible assets that have special economic properties (Mihet & Philippon, 2018). Parallel to the increasing availability of large amounts of data, in recent years there has been a remarkable advance at a theoretical level and in the practical applications of what can be done with the data. These advances, mainly focused on data science, are allowing the development of high-performance algorithms that help maximize the value of data for tourism firms. This has proven that Big Data is no longer considered a passing fad and has become a key tool for detecting patterns, understanding consumer behavior and satisfaction in a more accurate and deeper way, and predicting key variables, such as the arrival of tourists, hotel occupancy, or the profitability of firms (X. Li & Law, 2020).

Ironically, even though tourism firms have more data than ever before, only a small fraction of the data is actually serving a purpose. This is not surprising because the challenges facing any organization willing to extract value from data are considerable, such as how to handle large volumes of data and integrate dozens, if not hundreds, of different sources, and provide consistency to the various formats in which the data is stored. New technologies, such as cognitive computing, promise to address this challenge. These technologies are specifically designed to integrate and analyze large data sets and extract meaning from different types of data, thus representing a big leap forward from classical computing, since they mimic some aspects of human thought when it comes to evaluating information but without the biases introduced by human cognition (Castaldi et al., 2018).

In summary, Big Data promises exciting new opportunities, including unlocking new insights that can accelerate the creation of new products and services, boost customer relationships, improve operational processes, and even embrace innovative business models. Furthermore, Big Data is the perfect companion to traditional "small data". However, Big Data and all its potential will come to nothing if organizations are not capable of integrating it, analyzing it, and understanding it. And this is a highly complex process that goes beyond Big Data, as it transcends the mere capture and processing of information, and requires organizations to equip themselves with specialized techniques and knowledge with which to analyze the results and extract value from them.

## Concept of Big Data

It is not easy to define what Big Data is, or to draw a clear dividing line between what is "big" and "small" data. This difficulty is reflected in the number and variety of conceptualizations that can be found about Big Data in the literature.

Perhaps one of the most common and widely accepted ways of defining Big Data is through the "feature-oriented perspective", which characterizes Big Data according to three Vs: volume, variety, and velocity. Unlike other types of data, Big Data cannot be stored on an ordinary PC or portable hard drive, as it typically exceeds 100 terabytes or even petabytes of information (hence the volume). Big Data emanates from a wide variety of structured and unstructured sources, and can have many formats (e.g., texts, sounds, images, videos). In addition, the data is usually spatially and temporally referenced (variety). Likewise, the speed of data creation and analysis occurs in a very short time, which allows decision-making to be adapted to very short time windows that may lead to the initiation of corrective and/or adjustment actions practically in real time.

But these are not the only characteristics that help us define Big Data. The conceptualization based on the three Vs has been extended several times to accommodate four more Vs, as shown in Fig. 10.1 (Mariani et al., 2018).

-   **Veracity,** which refers to the reliability, validity, and completeness of data.

-   **Value,** which highlights the role of Big Data as a tool used to create value for both consumers and firms.

-   **Variability,** because Big Data often consists of unstructured records whose meaning can change depending on the moment and the context.

-   **Visualization,** or the need for the insights obtained from Big Data to be displayed in a visually attractive and understandable way for users so that they can do useful things with them.

In addition to the "feature-oriented perspective", Big Data can also be defined according to a "process-oriented perspective". This approach is based on highlighting the processes that are inherent to Big Data, such as the collection, storage, processing, and analysis of Big Data, and the technologies that support them. Using this approach, Big Data is the set of data that is difficult to collect, manage, analyze, and visualize in a limited time with the available technologies of today (Chen & Zhang, 2014). This entails the need for organizations to implement processes aimed at managing Big Data and technologies that overcome the limitations of traditional technologies.

Other characteristics that distinguish Big Data, apart from those mentioned in the "feature-oriented" and "process-oriented" perspectives, are the following:

Big Data often collects and explores entire populations rather than samples, thus challenging conventional statistical tools and inferential methods.

Big Data provides a high level of granularity in the data, which allows the analyst to focus on very fine aspects or very specific qualities of the information available, which can be reflected in finer and more accurate decision-making.

Big Data can be used flexibly when analyzing different data collections and extracting meaning from them.

Owners and managers should keep in mind that when talking about Big Data, we are not only referring to large public or internal transactional structured datasets (e.g., sales, customers, inventories, data from the population register, vehicle registration, etc.), but also peripheral and non-transactional unstructured data generated by sensors, smartphones, and radio-frequency identification (RFID) chips, which is used to track the dynamics of visitors in a territory, learn about the online behavior of consumers and their interactions in social media, and predict decisions and trends faster and more accurately (Morabito, 2015; Xu et al., 2020).

Big Data is also synonymous with data analytics and disruptive technologies (Unhelkar, 2017). Analytics has been around for some time now and has gone through various stages, from the invention of spreadsheets that allowed for simple calculations to today's sophisticated analytics tools. Data analytics relies heavily on statistical techniques, including both descriptive and predictive modeling, to understand not only "what has happened" but "what will happen". From a statistical point of view, Big Data analytics allows the identification of patterns, makes predictions, and provides advice for better decisions making (i.e., which products to deliver to customers, or which services to bundle). For their part, Big Data technologies allow complex analytics to be applied to large and highly dispersed data sets, providing very fine granularity and, in essence, leading to accurate decision-making. A technological framework widely used by firms that work with Big Data is Hadoop. The Hadoop ecosystem enables programmatic management of all kinds of data science-related tasks (using open source languages such as Java, Python, and R), ranging from data storage in a distributed database architecture to data manipulation, automation and business analytics.

## Big Data Technologies

Big Data would not exist without a strong pace of technological innovation. In recent years, there has been a real explosion in the availability of data as a result of the number of devices connected to the internet, which has increased exponentially. This phenomenon has attracted the attention of the technology industry and the interest of computer and data scientists, who work together to develop new methods that go beyond traditional data storage and management tools and are able to extract value from Big Data (Mariani et al., 2018).

As has happened in other industries, tourism has widely recognized the need to address these new technologies and use Big Data to implement a customer-centric approach that improves consumer experience and satisfaction. Big Data-based approaches solve many of the problems associated with working with representative samples of data, as they can encompass almost the entire population under scrutiny. In this way, Big Data becomes a powerful tool to address new and innovative research questions that can ultimately drive the creation of new value for tourism firms and their customers.

Compared to conventional relational database management systems, which are considered standard for structured data management, Hadoop and NoSQL are technological solutions focused on handling Big Data. Both are complementary and compatible with each other, but there are also substantial differences between them. The Hadoop framework is often used when data sizes are really big. Its origin was a published article on the design and implementation of the Google File System (Ghemawat et al., 2003), a scalable distributed file system for data-intensive applications, supporting a virtually unlimited number of computers in a network (nodes) to process petabytes of data simultaneously. Hadoop is currently a collection of open software utilities built for both storage and distributed parallel processing of data sets. It is available through the Apache distribution or from providers such as Cloudera, MapR, and HortonWorks. Hadoop is made up of four main components (Fig. 10.2).

-   **HDFS (Hadoop File System),** is the storage unit for large sets of structured and unstructured data and, as such, is the main component of the Hadoop ecosystem. With HDFS, data can be stored on thousands of machines (called nodes), and metadata can be maintained in the form of log files for fast and efficient access.

-   **MapReduce,** is the programming-based processing unit of the Hadoop ecosystem, which makes it possible to write applications that transform large data sets into manageable sets using parallel and distributed algorithms. Hadoop splits files into large blocks and then sends the packaged code to cluster nodes to process the data in parallel. With MapReduce the process is done on the slave nodes and the final result is then sent to a master node.

-   **YARN (Yet Another Resource Negotiator),** is the resource management unit of the ecosystem and the one that manages the resources in the clusters of nodes, programming and allocating resources for the Hadoop system and making sure the machines are not overloaded. Hadoop YARN operates as an operating system for Hadoop built on top of HDFS.

-   **Hadoop Common (also called Core),** consists of utilities and libraries to start Hadoop and support the rest of the modules.

Other tools and software packages that can be installed on top of or alongside Hadoop include Spark, Hive, Impala, Pig, Hive, HBase, Kudu, Kafka, ZooKeeper, Flume, Sqoop, Oozie, and Storm, all of which can work collectively to provide services such as data absorption, analysis, storage, and maintenance.

One of the main benefits of using Hadoop is that the firm can use business machines as data nodes, so the system is highly scalable and allows any data set to be processed faster and more efficiently than with a conventional computer architecture. This can lead to significant savings for firms since they do not have to invest thousands of dollars in very expensive data nodes. In addition, the Hadoop framework is written primarily in the Java language, with some native C code, giving users the flexibility to program it either with Java or any other non-Java programming language through Hadoop Streaming.

Depending on the specific needs of the firm, it may be convenient to use Hadoop and NoSQL separately, or together in mixed architectures, thus taking advantage of the characteristics of each one to find the most efficient answers. NoSQL databases (non-relational databases) are a more flexible and scalable solution than conventional relational databases as they are designed to manage and retrieve data on a massive scale in formats other than tables (as relational databases do). Since NoSQL databases are distributed databases (where data is stored on multiple servers), new data can be added to them without having to be defined upfront in the database schema, thus allowing rapid processing of big volumes of data of all kinds. In this way, NoSQL databases cope with the scalability and performance problems of conventional relational databases in situations of concurrence of thousands of users and millions of daily queries. As data continues to grow, simply add more hardware to keep up, without slowing down performance. Some of the most popular NoSQL platforms are MongoDB, Elasticsearch®, and Redis®.

Ultimately, NoSQL databases provide a different data management system than relational systems and the Hadoop framework, and, although both can operate autonomously, they are compatible. The integration between NoSQL and Hadoop systems is almost native, making it easy to integrate with Hadoop. Sometimes it can be very helpful to connect to Hadoop to do analysis from NoSQL, where the information is stored. This dynamic NoSQL database schema with Hadoop software is valuable for agile development when rapid and continuous iteration is required. Big Data and Agile can go hand in hand: Big Data allows the firm to be agile, and the agility provided through Hadoop-NoSQL gives the firm the capabilities to formulate successful Big Data strategies. As Big Data continues to grow, it is realistic to think that the combination of NoSQL database and Hadoop software will become a powerful framework that will allow firms to reach their full potential with data.

## Impacts and Opportunities
